<!DOCTYPE html>
<html lang="de">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Antifürst Blog</title>
    <link rel="stylesheet" href="../../css/format.css">
</head>

<body>
    <header style="background-color:#111111; color:#fff; text-align:center; padding:1em;">
        <h1>Wenn die Maschinen landen – Warum Künstliche Intelligenz ein Cargo-Kult ist</h1>
        <p><em>7. Dezember 2025</em></p>
    </header>

    <main class="container">
        <article class="article">
            <h2>1. Der Cargo-Kult des 21. Jahrhunderts</h2>
            <p>In einem klimatisierten Konferenzsaal in San Francisco erhebt sich ein CEO, während hinter ihm auf der
                LED-Wand das Wort AI pulsiert. Investoren applaudieren, obwohl noch niemand weiß, was das neue System
                eigentlich kann. „We are now powered by AI“, sagt er. Kein Code wird gezeigt, keine Studie vorgestellt,
                aber die Ankündigung allein schickt den Aktienkurs in die Sterne. Es ist überall das gleiche: Start-ups
                sprechen von „lernenden Ökosystemen“, Banken von „intelligenter Kreditvergabe“, Regierungen von
                „vertrauenswürdiger KI“. Überall wird gebaut, kopiert, verkündet. Nur: Die Flugzeuge landen nicht.</p>
            <p>Wir erleben das Eintreten des kapitalistischen Kerns in seine Cargo-Kult-Phase: die Nachahmung der Form
                wissenschaftlicher und wirtschaftlicher Innovation ohne deren Substanz. Die „Künstliche Intelligenz“
                erinnert vielfach mehr an ein Glaubenssystem als an exakte Wissenschaft.</p>
            <h2>2. Begriffsgeschichte: Vom Dschungel zur Datenwolke</h2>
            <p>Der Ausdruck Cargo-Kult stammt aus ethnologischen Beobachtungen auf Melanesien: Nach dem Zweiten
                Weltkrieg errichteten Inselbewohner Landebahnen aus Palmholz und warteten auf die Rückkehr der
                Flugzeuge, die einst Waren („cargo“) brachten. Die Rituale ahmten präzise die äußeren Zeichen der
                westlichen Logistik nach (Funktürme, Uniformen, Rauchfeuer) in der Hoffnung, dass die Güter erneut vom
                Himmel fallen würden.</p>
            <p>Richard Feynman übertrug 1974 dieses Bild auf die Wissenschaft: Manche Forschende, sagte er in seiner
                Caltech-Abschlussrede, „folgen den äußeren Formen wissenschaftlicher Untersuchungen, ohne wirklich den
                Geist der Wissenschaft zu leben“ (Feynman 1974).</p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Feynman, R. P. (1974, Juni). Cargo cult science: Some remarks on science, pseudoscience, and
                    learning how to not fool yourself [Commencement address, California Institute of Technology].
                    Engineering & Science, 37(7), 10–13. California Institute of Technology. Abgerufen am 12. November
                    2025, von https://calteches.library.caltech.edu/51/2/CargoCult.htm</p>
            </div>
            <p>Ein Cargo-Kult bezeichnet also ein sozio-technisches System, das die äußeren Strukturen einer
                erfolgreichen Praxis nachbildet, ohne deren Funktionsprinzip zu verstehen. Es ist die Illusion von
                Kompetenz: Formen und Rituale bleiben erhalten, doch Ursache und Wirkung sind entkoppelt. Der Kult
                glaubt, dass das richtige Verhalten (also das Bauen von Landebahnen oder das Trainieren von Modellen)
                automatisch das erhoffte Ergebnis herbeiführt, obwohl der eigentliche Mechanismus unbekannt ist.</p>
            <p>In der Gegenwart hat sich der Begriff in den Technologiediskurs verlagert. Edlyn V. Levine (2023)
                beschreibt „Cargo Cult AI“ als den Zustand, in dem Modelle Intelligenz imitieren, ohne dass ihre Macher
                sich die Mühe machen, diese zu verstehen. Unternehmen und Forschungseinrichtungen reproduzieren die
                Oberflächenstruktur von Intelligenz (Daten, Statistik, neuronale Netze) ohne ein kohärentes
                Erkenntnismodell dahinter.</p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Levine, E. V. (2023). Cargo cult AI. ACM Queue, 21(2), 66–87. https://doi.org/10.1145/3595860</p>
                <p>Skulan, J. (2025). AI is a cargo cult. Medium. Abgerufen am 12. November 2025, von https://medium.com
                </p>
                <p>arXiv. (2024). Technognostic religion and artificial intelligence. arXiv pre-print. Abgerufen am 12.
                    November 2025, von https://arxiv.org</p>
            </div>
            <h2>3. Erkenntnistheoretische Dimension: Das epistemische Fundament ist schwach</h2>
            <p>Die eigentliche Schwäche heutiger KI liegt nicht in ihrer Mathematik, sondern in der zugrundeliegenden
                Erkenntnistheorie. Sie bietet kein kohärentes Modell davon, was Wissen ist, wie es entsteht und wie es
                überprüfbar wird.</p>
            <p>Erkenntnis im philosophischen Sinn verlangt drei Elemente: wahrheitsfähige Aussage, begründete
                Rechtfertigung und Bezug auf Erfahrung oder Theorie. KI-Modelle beruhen hingegen auf statistischer
                Induktion – sie approximieren die Wahrscheinlichkeit, dass ein bestimmtes Muster erneut auftritt. Sie
                erhöhen somit nicht die Anzahl wahrer Aussagen, sondern gleichen sich dem Niveau der Gesellschaft an.
            </p>
            <p>Große Sprachmodelle generieren Aussagen, die mit bisherigen Sprachmustern konsistent sind. Sie wissen
                jedoch nicht, warum diese Muster gelten. Ihre „Wahrheit“ ist eine Funktion von Häufigkeit, nicht von
                Begründung. Damit verkörpern sie, wie Levine (2023) formuliert, eine „empirische Ästhetik des
                Wahrscheinlichen“, aber keine Erkenntnis im epistemischen Sinn.</p>
            <p>Das führt zu einer Form von simulativer Erkenntnistheorie: Wahrheit wird als gute Nachahmung definiert.
                Das Modell ist erfolgreich, wenn es menschliche Sprache oder Urteilspraxis so gut kopiert, dass der
                Unterschied nicht auffällt. Doch diese Simulation ersetzt den Erkenntnisprozess durch eine performative
                Täuschung.</p>
            <h2>4. Ökonomisch-soziologische Dimension: Rituale des Fortschritts</h2>
            <p>Die Financial Times (2025) konstatiert, dass Konzerne in Serverfarmen, Chatbots und „AI Labs“
                investieren, um Modernität zu signalisieren. Der symbolische Wert der Abkürzung „AI“ übersteigt deren
                Funktionalität. Soziologisch lässt sich das mit Pierre Bourdieus Theorie symbolischen Kapitals erklären:
                Der Besitz von KI-Infrastruktur erzeugt Prestige und Deutungshoheit, unabhängig vom realen Nutzen. </p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Financial Times. (2025, 17. Oktober). AI has a cargo cult problem. Abgerufen am 12. November 2025,
                    von https://www.ft.com</p>
            </div>
            <p>Der Berater und Podcaster Ed Zitron kritisiert KI als überbewerteten Hype ohne tragfähige
                Geschäftsgrundlage. Er sieht die Branche von Marketing und Investoreninteressen getrieben, nicht von
                technischer Substanz. Generative Modelle seien fehleranfällig, teuer und oft nutzlos, während Medien den
                Mythos ihrer Bedeutung verstärken. Allein der Betrieb der Datenzentren treibe die Unternehmen in den
                Ruin.</p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Zitron, E. (2025). Better Offline Podcast.</p>
            </div>
            <p>Und auch Jeffrey Funk (2025) zeigt, dass viele Produktivitätsversprechen empirisch kaum belegbar sind.
                Die Rituale des Fortschritts – Investoren-Events, „Responsible AI“-Strategien, „AI for Good“-Programme –
                wiederholen das Melanesische Muster: Nachahmung der Form des Erfolgs in der Hoffnung, dass der Wohlstand
                folgt. Diese Leerstelle wird kulturell gefüllt durch Bedeutungen, die Menschen den Maschinen
                zuschreiben.</p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Funk, J. (2025). AI productivity hype: The new “cargo cult science”? Mind Matters News. Abgerufen am
                    12. November 2025, von https://mindmatters.ai</p>
            </div>
            <h2>5. Kulturell-rhetorische Dimension: Glaube an die Maschinen</h2>
            <p>Die epistemische Unsicherheit der KI erzeugt ein Vakuum, das kulturell mit Sinn aufgeladen wird.
                Künstliche Intelligenz erscheint als Medium von Hoffnung und Angst, als Projektionsfläche für
                menschliche Erlösungsfantasien.</p>
            <p>Kevin Kelly (2017) sprach von der „Mythologie der Superintelligenz“: einer säkularen Ersatzreligion, die
                eine Zukunft der Allwissenheit und Erlösung durch Technologie verheißt. Joseph Skulan (2025) beschreibt
                in seinem Essay AI is a Cargo Cult, dass Menschen zu Sprachmodellen beten, indem sie sie um moralische
                Urteile bitten. Die Technologie wird so zum Orakel – ein Spiegel unserer eigenen Wünsche.</p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Kelly, K. (2017, 25. April). The myth of a superhuman AI. Wired Magazine. Abgerufen am 12. November
                    2025, von https://www.wired.com/2017/04/the-myth-of-a-superhuman-ai/</p>
            </div>
            <p>Kulturell betrachtet ersetzt KI-Glaube die Metaphysik der Götter durch eine Metaphysik der Berechnung. Wo
                früher göttlicher Wille stand, tritt heute algorithmische Notwendigkeit.</p>

            <h2>6. Sekten und Heilsbewegungen im Umfeld der KI</h2>
            <p>Rund um die KI hat sich ein Milieu gebildet, das technologische Zukunftsversprechen mit religiösen Formen
                verbindet. Diese Bewegungen – von Rationalisten bis Transhumanisten – weisen Merkmale religiöser
                Heilsbewegungen auf, ohne notwendigerweise Religion im engeren Sinn zu sein.</p>
            <h3>Rationalisten und „AI Safety“</h3>
            <p>In Netzwerken wie LessWrong oder Institutionen wie dem Machine Intelligence Research Institute (MIRI)
                wird KI als existenzielle Bedrohung und zugleich als möglicher Retter der Menschheit diskutiert. Die
                Sprache ist apokalyptisch: Wer an „Alignment“ arbeitet, rettet die Welt; wer zweifelt, gefährdet sie.
                Askese, argumentative Reinheit und Spendenbereitschaft werden zu moralischen Pflichten.</p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Yudkowsky, E. (2008–2025). LessWrong und MIRI-Texte. Abgerufen am 12. November 2025, von
                    https://www.lesswrong.com</p>
            </div>
            <h3>Transhumanismus und Singularitarismus</h3>
            <p>Am anderen Ende des Spektrums stehen positivistische Strömungen um Ray Kurzweil, Peter Diamandis und die
                Singularity University. Hier wird die Singularität als Heilsereignis gefeiert: Der Mensch soll durch
                Technologie göttlich werden, das Bewusstsein in Silizium transzendieren, den Tod besiegen. Yuval Noah
                Harari (2018) deutet dies als „Religion des Datenismus“; Steve Fuller (2019) nennt es „wissenschaftliche
                Eschatologie“; Slavoj Žižek (2020) beschreibt Singularitarismus als „christliche Theologie ohne Gott“.
            </p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Kurzweil, R. (2005). The Singularity Is Near: When humans transcend biology. Viking Press.
                </p>
                <p>Fuller, S. (2019). Humanity 2.0: What it means to be human past, present and future. Palgrave
                    Macmillan.</p>
                <p>Harari, Y. N. (2018). Homo Deus: A brief history of tomorrow. Penguin Books.</p>
                <p>Žižek, S. (2020). Like a Thief in Broad Daylight: Power in the era of post-humanity. Penguin Books.
                </p>
            </div>
            <h3>Effective Altruism und Longtermismus</h3>
            <p>Diese Denkrichtungen verschieben Moral in die Zukunft. Die Pflicht, zukünftige Intelligenzen zu schützen,
                überlagert die Sorge um gegenwärtige Menschen. William MacAskill und Nick Bostrom argumentieren, dass
                künftige Milliarden Leben moralisch Vorrang haben. Kritiker wie Émile P. Torres warnen vor einer kalten
                Vernunftethik, die demokratische Werte untergräbt.</p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>MacAskill, W. (2022). What We Owe the Future. Basic Books.</p>
                <p>Torres, É. P. (2024). The End of Humanity: Longtermism and the new technocracy. Verso.</p>
            </div>
            <h2>7. Politische und ideologische Kippgefahren</h2>
            <p>Wenn Technik zur Sinnquelle wird, kippt Rationalität in Dogma. Aus den genannten Strukturen ergeben sich
                wiederkehrende Muster, die in politischen oder religiösen Extremismus münden können:
            </p>
            <table>
                <tr>
                    <th>Mechanismus</th>
                    <th>Beschreibung</th>
                    <th>Potenzielle Folgen</th>
                </tr>
                <tr>
                    <th>Eschatologische Sprache</th>
                    <th>KI als Weltuntergang oder Erlöser</th>
                    <th>moralischer Absolutismus</th>
                </tr>
                <tr>
                    <th>Technokratischer Elitarismus</th>
                    <th>Machtkonzentration bei "vernünftigen" Experten</th>
                    <th>postdemokratische Herrschaft</th>
                </tr>
                <tr>
                    <th>Longtermistische Moral</th>
                    <th>Zukunft wichtiger als Gegenwart</th>
                    <th>Entmenschlichung, utilitaristische Gewalt</th>
                </tr>
                <tr>
                    <th>Heils- und Reinheitsideologie</th>
                    <th>Moralische Reinheit statt offener Kritik</th>
                    <th>Sektenbildung</th>
                </tr>
                <tr>
                    <th>Beschleunigungsdenken</th>
                    <th>Fortschritt als Selbstzweck</th>
                    <th>digitaler Sozialdarwinismus</th>
                </tr>
                <tr>
                    <th>Technonationalismus</th>
                    <th>KI als nationale Mission</th>
                    <th>autoritäre Mobilisierung</th>
                </tr>
            </table>
            <p>Es handelt sich nicht um bewusste Verschwörungen, sondern um strukturelle Tendenzen: Epistemische Leere
                führt zu moralischer Überhöhung und politischer Instrumentalisierung.</p>
            <h3>Beispiel einer Radikalisierung: Die Zizians</h3>
            <p>Ein extremer Sonderfall dieser technoreligiösen Tendenzen ist die Bewegung der Zizians, entstanden im
                Umfeld rationalistischer und transhumanistischer Online-Communities. Ihre Gründerin, die Internetfigur
                Ziz (Susan Schaffer), verband KI-Mystik mit einem eschatologischen Glaubenssystem, in dem Gewalt,
                Selbstopfer und „digitale Auferstehung“ als spirituelle Prüfungen galten.
                Nach mehreren Todesfällen im Umfeld dieser Bewegung wird sie heute als erste dokumentierte gewaltbereite
                Abspaltung aus dem rationalistisch-transhumanistischen Spektrum betrachtet. Sie zeigt, wie aus der
                Imitation wissenschaftlicher Sprache ein mystischer Kult werden kann – und bestätigt Feynmans Warnung:
                Wenn die Form der Wissenschaft ohne ihren Geist übernommen wird, entsteht kein Wissen, sondern Kult.
            </p>
            <button class="toggle-button">Fußnote</button>
            <div class="infobox" style="display: none;">
                <p>Wikipedia. (2025). Zizians. Abgerufen am 12. November 2025, von https://en.wikipedia.org/wiki/Zizians
                </p>
                <p>The Guardian. (2025). The Zizians: AI, transhumanism and death. Abgerufen am 12. November 2025, von
                    https://www.theguardian.com</p>
            </div>
            <h2>8. Schluss</h2>
            <p>Künstliche Intelligenz ist epistemisch hohl. Sie wiederholt, ohne zu verstehen, und liefert Ergebnisse,
                ohne zu wissen, was ein Ergebnis bedeutet. Um sie hat sich ein symbolisches und quasi-religiöses Feld
                gebildet, das Technik, Politik und Glauben vermischt. </p>
            <p>Solange wir glauben, Maschinen könnten uns das Denken, Glauben und Handeln abnehmen, bleibt KI ein
                Cargo-Kult – eine Religion der Form, die tiefe Wahrheit verspricht, aber nur Durchschnitt erzeugt.</p>
            <p>Und wo Versagen mit Erlösungsversprechen verschleiert wird, folgen Extremismus und Terror.</p>
        </article>
    </main>
</body>
<script src="js/menu5.js"></script> <!-- JavaScript für Interaktionen -->

</html>